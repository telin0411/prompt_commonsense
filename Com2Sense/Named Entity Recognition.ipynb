{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ade97dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poland GPE\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"Treblinka is a small village in Poland. Treblinka was also an extermination camp.\"\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.create_pipe(\"entity_ruler\")\n",
    "\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "@Language.component(\"myruler\")\n",
    "def myruler(ruler):\n",
    "    return ruler\n",
    "\n",
    "#add the pipe to the model\n",
    "nlp.add_pipe('myruler')\n",
    "\n",
    "#create the doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd3292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a car']\n",
      "drive VERB\n",
      "a DET\n",
      "car NOUN\n",
      ". PUNCT\n",
      "a car\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Since Erica is afraid of heights, she decided to fly on an airplane to her aunt\\u2019s house three states away rather than drive a car. \"\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "noun = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "word_pos = {token.text: token.pos_ for token in doc}\n",
    "\n",
    "for word in noun:\n",
    "    if word in word_pos and word_pos[word] == 'PRON':\n",
    "        noun.remove(word)\n",
    "\n",
    "print(noun)\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea0391e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['her aunt’s house', 'a car']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "def find_noun(text: str, side: str):\n",
    "    \n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    words_dic = {token.text: token.pos_ for token in doc}\n",
    "\n",
    "    noun_list = [chunk.text for chunk in doc.noun_chunks]\n",
    "    \n",
    "    if side == 'left':\n",
    "        noun_list.reverse()\n",
    "    \n",
    "    for noun in noun_list:\n",
    "        if noun not in words_dic or words_dic[noun] != 'PRON':\n",
    "            return noun\n",
    "    return ''\n",
    "                                        \n",
    "    \n",
    "def find_comparision(sentence: str):\n",
    "    \n",
    "    # We require the input sentence must contain only one 'than':\n",
    "    if sentence.count(' rather than ') == 1:\n",
    "        # split the sentence into 2 part by 'than'\n",
    "        sentence = sentence.split(' rather than ')\n",
    "        sentence_l = sentence[0]\n",
    "        sentence_r = sentence[1]\n",
    "    \n",
    "        last_noun_l = find_noun(sentence_l, 'left')\n",
    "        first_noun_r = find_noun(sentence_r, 'right')\n",
    "    \n",
    "        return [last_noun_l, first_noun_r]\n",
    "    \n",
    "    else:\n",
    "        return ['', '']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_sentence = \"Since Erica is afraid of heights, she decided to fly on an airplane to her aunt\\u2019s house three states away rather than drive a car.\"\n",
    "    print(find_comparision(input_sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc390e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['her aunt’s house', 'an airplane'] Since Erica is afraid of heights, she decided to drive a car to her aunt’s house three states away rather than fly on an airplane. \n",
      "['a six foot cord', 'a one foot cord'] It is easier to move around more freely while using your cell phone if it is charging with a six foot cord rather than a one foot cord. \n",
      "['a plastic grocery bag', 'an oven mitt'] If you lost your mitten in a snowbank, a proper alternative would be a plastic grocery bag rather than an oven mitt.\n",
      "['the next size', 'the next size'] If the store doesn’t have your exact shoe size, it is generally better to buy the next size up rather than the next size down. \n",
      "100\n",
      "['a cold sunny day', 'a warm rainy day'] The family's drier is broken so it's better to wait for a cold sunny day rather than a warm rainy day to wash their clothes.\n",
      "['a rag', 'a strap'] It's more convenient to use a rag rather than a strap, as a dusting cloth.\n",
      "['a plane', 'a boat'] To travel to Iceland from California, it is faster to fly on a plane rather than take a boat.\n",
      "['meat', 'vegetables'] Cats are more likely to feel satisfied after eating meat rather than vegetables. \n",
      "['a taxi', 'work'] If your car does not start, it may be better to take a taxi rather than to run to work.\n",
      "['a LCD reusable blackboard', 'sheets'] In order to make our part in saving the planet, it is recommend to write our notes on a LCD reusable blackboard rather than sheets of paper.\n",
      "200\n",
      "['a weeknight', 'Matt'] Jeff has 200 friends that he talks to regularly, while Matt only has 2.  Jeff is more likely to be at a party on a weeknight rather than Matt.\n",
      "['his teammate', 'Jack'] Andrew and Jack will be playing football on opposing teams. Andrew will more likely pass the ball to his teammate rather than to Jack.\n",
      "['the animal', 'the steakhouse'] As she contemplates going vegan, Beth looks for inspiration by deciding to visit the animal sanctuary rather than the steakhouse.\n",
      "['the microwave', 'the wood burning stove'] Daniel had a train to catch in an hour so he heated his soup in the microwave rather than fire up the wood burning stove.\n",
      "300\n",
      "['the next month', 'the next year'] Sarah starts college in 3 months, so she should choose her classes in the next month rather than the next year.\n",
      "['1 week', '20 weeks'] It is safer to drink milk after it's been refrigerated for 1 week rather than 20 weeks.\n",
      "['Europe', 'next month'] Maggie in the US sent a letter to her boyfriend living in Europe. She expects it to arrive today rather than next month.\n",
      "['2 hours', '15 minutes'] Amy wanted to make a pizza for dinner so she put the pizza in the oven for 2 hours rather than 15 minutes.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('/nas/home/yixiaoli/prompt_commonsense/com_combine/datasets/com2sense/dev.json')\n",
    "\n",
    "df_new = pd.DataFrame(columns = df.columns)\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if index % 100 == 0:\n",
    "        print(index)\n",
    "    entities_1 = find_comparision(row['sent_1'])\n",
    "    \n",
    "    entities_2 = find_comparision(row['sent_2'])\n",
    "    \n",
    "    if '' not in entities_1 and '' not in entities_2:\n",
    "        print(entities_1, row['sent_1'])\n",
    "        row['sent_1'] = f\"{entities_1[0]}, {entities_1[1]}. {row['sent_1']}\"\n",
    "        row['sent_2'] = f\"{entities_2[0]}, {entities_2[1]}. {row['sent_2']}\"\n",
    "        cnt += 1\n",
    "        df_new = df_new.append(row)\n",
    "\n",
    "#df_new.to_json('/nas/home/yixiaoli/prompt_commonsense/Com2Sense/datasets/com2sense/test_rather_than.json', orient='records')\n",
    "print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96f199b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-large and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ca8437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
